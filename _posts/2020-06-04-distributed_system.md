---
layout: post
title: "一个研发工程师在短视频领域做了什么（二）"
---
我过去一段时间的工作经历是开发支撑一款千万级日活的用户产品（to C）的后台，和企业级（to B）产品不同，用户产品的后台没有过于复杂的业务逻辑，但却需要应对大量用户同时访问可能带来的高并发请求对系统的冲击。面对高并发的冲击，业务系统如何保证其业务逻辑正确性的基础上，同时保证服务的响应速度，是衡量系统可用性的关键。这里通常有两个指标来衡量：吞吐量和延迟。吞吐量指的是单位时间能处理多少请求，高并发请求通常也就意味着高吞吐量。延迟指的是一个请求需要耗费多长时间得到响应。当有大量的上游用户请求同时访问系统（吞吐量增加），请求量增加到瓶颈后，由于计算资源的限制，请求就会相互抢占资源，导致指标的下降。也就是说高并发系统需要保证系统有高吞吐量和低延时。

# 如何优化

## 优化实现
这是在进行代码开发时所可以进行的优化，这些优化都是最核心最底层的涉及到硬件资源的利用效率，直观上就是项目投资的每一分钱发挥多大的价值：

* **算法优化**：这是工程师的基本能力，我们在面试时之所以考察算法和数据结构的能力，就是要在开发时尽可能减少时间复杂度和空间复杂度，以节省计算和存储资源。
* **语言优化**：不同语言有不同的特点，有的语言为了灵活性放弃了性能如python，有的语言性能强大却难以驾驭如c，针对不同的系统诉求选择合适的语言进行开发时十分重要的。

## 扩展资源
除了提高资源的利用率之外，更直接有效的解决这个问题方式就是增加投入，扩展资源。这里的资源，可能包含CPU、内存、存储等。然而，资源的扩展不是简单的叠加，是有诸多限制的。对于一台计算机上，CPU、内存、IO等资源在操作系统的组织下协调运行，但受体系结构限制，扩展资源是有极限的。此时，要继续扩展资源需要通过网络连接多个计算机，组成分布式系统，这样可以使用的资源就可以扩展多个计算机。但分布式系统引入网络传输，资源调度的方式与仅仅是操作系统调度有了很大不同，不仅是分配到哪个CPU或者哪个核，在这之上还要知道调度到哪个机器节点，这个协调的工作就需要额外的算法和服务来完成。同时系统组件增多，复杂程度提高，容错性能也是其能否达成提高高并发性能的重要因素。

* **有状态扩展**: 内存、磁盘这些存储资源通常会与请求相关的数据挂钩，也就是说系统中不同的节点只保留着部分数据，对应的请求也只能访问这些数据。此时，扩展节点资源必然会影响数据的分布，这就涉及到了不同的数据分布、迁移的策略。通常我们使用sharding的方式去设计这样的结构，保证不同数据合理的分配到对应的节点上。

* **无状态扩展**: 当扩展的资源不需要存储每个请求的状态时，那么所有的资源都是对等的，使用哪些来处理请求是没有区别的。此时我们通常使用replication的方式，直接复制不同节点的配置扩展到新节点上。

在不同场景下，sharding或是replication的方式应用不是互斥的，大部分场景会融合不仅保证扩展性还要保证容错性。

## 资源的调度
如之前所说，系统资源单机扩展成网络，调度资源也不仅仅只依靠操作系统完成。这里的调度，不仅仅指流量的调度，也包含资源如何初始化等其他内容。首先就涉及到资源的naming问题，最简单的方式就是使用IP地址和端口号进行定位从而知道有哪些资源可供我们调度。之后就需要管理这些节点资源的增加、删除、修改。完成配置后，分配对应的流量到相应的资源上。而在系统中，承担调度的角色，通常可以分为两种：
* **中心化**: 系统中有专门的节点进行调度。

* **非中心化**：系统中每个节点都能可以进行调度。

这两种方式各有利弊，中心化的方式更方便管理但可能出现单点失败，而非中心化的方式保证了稳定性但需要保证数据一致的overhead。

# 容错性
实际上容错性是分布式系统引入复杂性之后的overhead，尤其是在引入了更多的网络传输之后，各个组件fail的机会会增加。此时，容错性主要考量的是节点fail back和fail over：fail back指的是节点失败之后能够自动恢复；fail over指的是节点失败之后有相应的备援能够接受其工作。提高容错性就是围绕着两点来展开。

## 监测fail
通常我们说的是client-server的架构模式，那么我们怎么知道server故障了呢？最根本的，server内部有很多指标可以说明潜在的故障的发生，client也可以则大多时候可以通过timeout和其他明确的错误来感知。这样的检测方式会出现false positive，使得正常的节点被摘除，需要一段时间的累计

## 解决fail
解决fail问题的核心就是冗余，可以是空间和时间两个维度。这是很直观的方法，时间冗余指的是如果短时间失败可以重试，给系统有fail over的机会；空间冗余就是资源在不同的物理空间备份，给系统fail back的条件。当然这样的冗余也不是无限的，容错所带来的可用性是和响应时间相关的，过多的冗余会给系统产生overhead，极有可能使latency大幅延长。例如过多的重试会加剧系统资源的占用导致流量放大，服务器雪崩。而空间的冗余会增加保证数据一致性的开销。所以在实际场景中，解决容灾问题是在各种复杂因素综合评定下的tradeoff，并没有万能的策略解决所有的问题。

# 实践
## Golang
我们

## load balance
一套完成的web应用系统，又是有各个微服务系统做成的，最外层首先DNS中的域名会解析到多个出口ip，DNS服务器通常会采用round robin的算法，把流量调度到不同的ip上。
即便是这样，每个ip上并非是直接接服务器，会线上一个4层F5硬件，和7层nginx调控，这之后所有的，所有的都是使用docker服务化的，每一个节点会注册的consul进行服务发现。
数据库所用的都是各自的集群配置。redis使用temproxy+cluster的方式，mysql使用mycat做proxy转发流量

## 冗余备份
正常情况下，所有的服务都是50%的荣誉的。 redis和mysql都是采用主从备份，三机房冗余。我们会关注每个服务接口的流量变换

## 监控
基于Grafana和opentsdb搭建的实时监控系统，所有基础设施的CPU，也有所有的业务流量数据，我们通常需要关注每个接口的流量变化， 请求的延迟的avg和pct99值，通过观察指标是否符合预期，来判断系统的状态。我们会设定一些阈值指标来熔断和拒绝从而做到，同志

## 容灾演练
etcd的配置，基于IP-tables的

# 总结
算法和数据结构的时间和空间复杂的估算都没有考虑系统资源的限制，而在实际的生产环境中，尤其是在高并发请求的压力下，时间和空间复杂度的优化需要纳入资源这一重要维度进行考量。高并发的应用，需要建立在操作系统之上分布式的资源调度能力，来保证系统应对流量增加时的高扩展性和稳定性，在生产环境中，多因素作用的场景增加了系统设计的困难。